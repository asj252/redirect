{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87fd90c0-9da0-4340-8ca5-4d515d2a8da5",
   "metadata": {},
   "source": [
    "# Linear Algebra Crash Course (with Python) Class 9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30735ff9-ee24-4e67-b953-a02128a1e883",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Chap 8 Conti\n",
    "\n",
    "## Example of linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f5b316-3562-4e62-8be8-d44ed4c127f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 线性回归模块与伪逆矩阵之间的关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "e683a55b-e12a-4fb2-a7c8-4be2a3b06fca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "31360e96-6c78-48f8-8ba7-99b1b9480276",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1]\n",
      " [1 2]\n",
      " [2 2]\n",
      " [2 3]]\n",
      "[ 6  8  9 11]\n"
     ]
    }
   ],
   "source": [
    "## example1 \n",
    "M = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
    "# y = 1 * m_0 + 2 * m_1 + 3\n",
    "y = np.dot(M, np.array([1, 2])) + 3\n",
    "print(M)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "882487ab-08c1-4344-b352-c5228c28e2f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = LinearRegression().fit(M, y)\n",
    "reg.score(M, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "93395244-f8fe-4ed6-a276-b43a8b03d71d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2.])"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45a0fc56-79dc-45a0-aec1-dad4cd1926d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0000000000000018"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "797ed64a-2936-4ef6-aee8-29db6e6f2d97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.predict(np.array([[3, 5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92724353-28bc-47f6-b673-e5863c813aaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "M2= np.append(M,[[1],[1],[1],[1]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fafef5ff-085f-4da3-9f89-2c6c627fe8c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2., 3.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.pinv(M2).dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "7b23b0d7-7c7e-4cb9-8048-708de1fe431c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## example 2 \n",
    "M = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
    "M1 = np.array([M[:,0]**2,M[:,1]**3]).T\n",
    "M2 = np.array([M[:,0]**2,M[:,1]**3,[1,1,1,1]]).T\n",
    "\n",
    "# y = 1 * (m_0)^2 + 2 * (m_1)^3 + 3 + noise\n",
    "y=  M[:,0]**2 + 2*M[:,1]**3 + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "20a57128-1d31-4245-b9cd-038cb38203c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  1,  1],\n",
       "       [ 1,  8,  1],\n",
       "       [ 4,  8,  1],\n",
       "       [ 4, 27,  1]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "34ef3243-e8dc-4495-bd7c-572f5ff96b9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6, 20, 23, 61])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "eb1ef65f-9db0-4e94-a6cb-8582e55c30dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.17764712,  2.49896375, -0.74851921,  1.62703593])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5*( np.random.rand(4)-0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "528a9e95-15ce-4090-a2d1-73cf560f2a79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "yrand = y + 5*( np.random.rand(4)-0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "eefc6b1e-7a14-4ee0-ad55-cbd8ba55da95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = LinearRegression().fit(M1, y)\n",
    "reg.score(M1, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "bc5c840b-5330-4584-b7b4-89ada30455b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9987704742979117"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = LinearRegression().fit(M1, yrand)\n",
    "reg.score(M1, yrand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "ac12c384-2a01-406c-9d18-35a0cf6d7fec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.52052596, 2.15722953])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "17980709-c764-4751-acde-47fc6cf537cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.6463959472671554"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "afb84ebb-1d02-4108-9a33-138a59de865e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.52052596, 2.15722953, 2.64639595])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.pinv(M2).dot(yrand)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dbdd3f-4ce5-458c-bf3d-431a7712571e",
   "metadata": {},
   "source": [
    "## Chap 9 特征值、特征向量、特征方程式\n",
    "\n",
    "### 特征方程式\n",
    "我们***可以定义***一个方阵 $A$ 的 $\\lambda$ 特征多项式 $\\mathtt{det}(A-\\lambda \\boldsymbol{I})$，以及特征方程式 $\\mathtt{det}(A-\\lambda \\boldsymbol{I})=0$。\n",
    "不难检查 $\\lambda$ 特征多项式的最高次方为 $n$。\n",
    "\n",
    "\n",
    "可以考虑如果有$\\lambda$ 使得 $\\mathtt{det}(A-\\lambda \\boldsymbol{I})=0$，则 $A-\\lambda \\boldsymbol{I}$ 不可逆，则 $\\exists\\;  \\mathbf{ v}_{\\lambda}\\neq0\\in \\mathbb{R}^n \\Rightarrow(A-\\lambda \\boldsymbol{I})\\mathbf{  v}_{\\lambda}=0 \\Rightarrow A\\mathbf{  v}_{\\lambda}=\\lambda\\mathbf{  v}_{\\lambda}$。我们称 $\\lambda$ 是是一个特征值，以及 $\\mathbf{  v}_{\\lambda}$是对应的特征向量。\n",
    "\n",
    "**代数基本定理：任何一个非常数、单变数、复数系数方程式，都至少有一个复数解**\n",
    "\n",
    "> 高斯一辈子对**代数基本定理** 给出了4 种证明，有复分析、拓扑学、积分、代数等等。同时 **代数基本定理** 是高斯 22 岁的博士论文。\n",
    "\n",
    "假设 $A$ 是一个实系数方阵，则显然 $\\mathtt{det}(A-\\lambda \\boldsymbol{I})=0$ 是个实系数方程式，因此我们可以引用代数基本定理，*确保解一定存在*。更进一步对整个方程式取共轭时，可得到当 $\\lambda$ 是一个特征值因为 $\\mathbf{v}_{\\lambda}$ 是它的特征向量，它的共轭 $\\bar{\\lambda}$ 也是一个特征值， $\\bar{\\mathbf{v}}_{ \\lambda}$ 是它的共轭特征向量。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1e9916-43f2-44a3-82d0-c360ae18051d",
   "metadata": {},
   "source": [
    "### 对角线化矩阵\n",
    "\n",
    "如果一个 $n\\times n$ 方阵 $\\mathbf{A}$，有 $n$ 个相异的特征值 $\\{\\lambda_i\\}$，以及对应的特征向量$\\{\\mathbf{v}_i\\}$。\n",
    "這些$\\{\\mathbf{v}_i\\}$都是線型獨立的。考慮 $n=2$，结论显然成立。使用数学归纳法以及反證法，考虑$n=j+1$，若 $\\mathbf{v}_{j+1}=\\sum x_i\\mathbf{v}_{i}$。我们有 $\\lambda_{j+1}\\mathbf{v}_{j+1}=\\sum \\lambda_{j+1} x_i\\mathbf{v}_{i}$。类似的作用$A$在等式兩邊得到 $\\lambda_{j+1}\\mathbf{v}_{j+1}=\\sum \\lambda_{i} x_i\\mathbf{v}_{i}$。因此 $\\sum (\\lambda_{i}-\\lambda_{j}) x_i\\mathbf{v}_{i}=0$, 跟$n=j$ 時，$\\{\\mathbf{v}_i\\}$ 線型獨立的前提矛盾。\n",
    "\n",
    "因此我们有\n",
    "$$\\mathbf{A}=\\mathbf{VDV}^{-1}$$\n",
    "或\n",
    "$$\\mathbf{AV}=\\mathbf{VD}$$\n",
    "其中$\\mathbf{V}$ 的列向量是$\\{\\mathbf{v}_i\\}$，$\\mathbf{D}=（\\delta_{ij}d_{ij}）$ 是对角线矩阵,$d_{ii}=\\lambda_i$。\n",
    "\n",
    "问题在于特征值$\\{\\lambda_i\\}$，可能不到 $n$ 个。\n",
    "比如令\n",
    "$$A=\\begin{bmatrix}\n",
    "0 & 0 & 0 &0\\\\\n",
    "1 & 0 & 0 &0 \\\\\n",
    "0 & 1&0& 0\\\\\n",
    "0 & 0 & 1 &0\\\\\n",
    " \\end{bmatrix}$$\n",
    "我们发现 $A^4=0$,因此 $A$ 的特征值只可能是0，但$A\\mathbf{v}=0$,只有一个解 $[0,0,0,1]^T$, 并且我们有 $A^4[1,0,0,0]^T=A^3[0,1,0,0]^T=A^2[0,0,1,0]^T=A[0,0,0,1]^T=0$。就好像甲板上站了四个人，每次往前走一步，都有一个人会跳进水里。\n",
    "简单操作一下 $A$ 得到 $A+\\lambda\\mathbf{I}$，我们可以得到特征值为 $\\lambda$ 的矩阵，但一样只有一个特征向量。  Jordan Canonical form.\n",
    "事实上与其说一般的矩阵都可以对角化，不如说我们举的特例这更接近于一般的情况。但能对角化的确有很多好处，所以我们下面讨论特殊可对角化的矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec67195e-acbd-479c-b6f3-5506abe21c3c",
   "metadata": {},
   "source": [
    "\n",
    "### 复向量的内积\n",
    "\n",
    "我们首先扩充一下内积的定义范围，从 $\\mathbb{R}^n\\times\\mathbb{R}^n\\rightarrow \\mathbb{R}$, 到 $\\mathbb{C}^n\\times\\mathbb{C}^n\\rightarrow \\mathbb{C}$. 在复向量空间中，定义维度为 $n$ 的两个复向量 $\\mathbf{v}$ 和 $\\mathbf{w}$ 的内积为：\n",
    "\n",
    "$\\langle \\mathbf{v},\\mathbf{w} \\rangle = \\sum_{i=1}^n v_i \\overline{w_i}$\n",
    "\n",
    "其中 $v_i$ 和 $w_i$ 分别表示 $\\mathbf{v}$ 和 $\\mathbf{w}$ 的第 $i$ 个分量，$\\overline{w_i}$ 表示 $w_i$ 的复共轭。\n",
    "\n",
    "这个内积满足以下性质：\n",
    "\n",
    "1.  第一个参数的线性性：对于任意复常数 $\\alpha$ 和 $\\beta$ 以及复向量 $\\mathbf{v}$、$\\mathbf{u}$ 和 $\\mathbf{w}$，有 $\\langle \\alpha \\mathbf{v} + \\beta \\mathbf{u}, \\mathbf{w} \\rangle = \\alpha \\langle \\mathbf{v}, \\mathbf{w} \\rangle + \\beta \\langle \\mathbf{u}, \\mathbf{w} \\rangle$。\n",
    "2.  共轭对称性：对于任意复向量 $\\mathbf{v}$ 和 $\\mathbf{w}$，有 $\\langle \\mathbf{v}, \\mathbf{w} \\rangle = \\overline{\\langle \\mathbf{w}, \\mathbf{v} \\rangle}$。\n",
    "3.  正定性：对于任意非零复向量 $\\mathbf{v}$，有 $\\langle \\mathbf{v}, \\mathbf{v} \\rangle > 0$。\n",
    "\n",
    "\n",
    "需要注意的是，这种定义方式的内积也诱导出了一个复向量的距离，即 $|\\mathbf{v}| = \\sqrt{\\langle \\mathbf{v}, \\mathbf{v} \\rangle}$。\n",
    "\n",
    "原本我们有$\\langle \\mathbf{A}^T\\mathbf{v},\\mathbf{w}\\rangle=\\langle \\mathbf{v},\\mathbf{A}\\mathbf{w}\\rangle,\\; \\forall \\mathbf{v},\\mathbf{w}\\in \\mathbb{R}^n$。我们现在有 $\\langle \\mathbf{A}^*\\mathbf{v},\\mathbf{w}\\rangle=\\langle \\mathbf{v},\\mathbf{A}\\mathbf{w}\\rangle,\\; \\forall \\mathbf{v},\\mathbf{w}\\in \\mathbb{C}^n$，其中$\\mathbf{A}^*=\\bar{\\mathbf{A}}^T$，意即 $\\mathbf{A}$ 转置后取共轭，或共轭后取转置。因此  $\\langle \\bar\\lambda\\mathbf{v},\\mathbf{w}\\rangle=\\langle \\mathbf{v},\\lambda \\mathbf{w}\\rangle$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27666d4c-08fb-4fad-a620-377a52c3ec8f",
   "metadata": {},
   "source": [
    "\n",
    "### 对称矩阵的特征值与特征向量\n",
    "\n",
    "假设 $\\mathbf{S}$ 是一个实系数对称矩阵（方阵），显然 $\\mathbf{S}^*=\\mathbf{S}$。考虑我们有一个复特征值 $\\lambda$，以及对应的复特征向量 $\\mathbf{v}_{\\lambda}\\in \\mathbb{C}^n$。我们考虑 $\\bar{\\lambda}\\langle \\mathbf{v}_{\\lambda},\\mathbf{v}_{\\lambda}\\rangle=\\langle \\mathbf{v}_{\\lambda},\\lambda\\mathbf{v}_{\\lambda}\\rangle=\\langle \\mathbf{v}_{\\lambda},\\mathbf{S}\\mathbf{v}_{\\lambda}\\rangle=\\langle \\mathbf{S}^*\\mathbf{v}_{\\lambda},\\mathbf{v}_{\\lambda}\\rangle=\\langle \\mathbf{S}\\mathbf{v}_{\\lambda},\\mathbf{v}_{\\lambda}\\rangle=\\langle \\lambda\\mathbf{v}_{\\lambda},\\mathbf{v}_{\\lambda}\\rangle=\\lambda\\langle \\mathbf{v}_{\\lambda},\\mathbf{v}_{\\lambda}\\rangle$\n",
    "因此 $\\bar{\\lambda}=\\lambda$，实对称方阵的特征值一定是实数，并且可推知特征向量也都是实向量。\n",
    "\n",
    "我们也可以不使用复空间内积符号。 考虑 $\\mathbf{S}\\mathbf{v}_{\\lambda}=\\lambda \\mathbf{v}_{\\lambda}$,取转置共轭我们得到 $\\bar{\\mathbf{v}}_{\\lambda}^T \\mathbf{S}^*=\\bar{\\mathbf{v}}_{\\lambda}^T\\mathbf{S}=\\bar{\\lambda} \\bar{\\mathbf{v}}_{\\lambda}^T$。\n",
    "使用矩阵乘法，我们得到 $\\bar{\\lambda} \\bar{\\mathbf{v}}_{\\lambda}^T \\mathbf{v}_{\\lambda}=(\\bar{\\mathbf{v}}_{\\lambda}^T\\mathbf{S}) \\mathbf{v}_{\\lambda}=\\bar{\\mathbf{v}}_{\\lambda}^T\\mathbf{S} \\mathbf{v}_{\\lambda}=\\bar{\\mathbf{v}}_{\\lambda}^T(\\mathbf{S}\\mathbf{v}_{\\lambda})=\\lambda\\bar{\\mathbf{v}}_{\\lambda}^T\\mathbf{v}_{\\lambda}$。因此 $\\lambda$ 是实数。\n",
    "\n",
    "得知对于对称矩阵，特征值、特征向量都是实数后，我么可以暂时忘记复空间内积。\n",
    "\n",
    "考虑对称矩阵 $S$ 两个不同的特征值 $\\lambda，\\gamma$ 以及其对应的特征向量 $\\mathbf{v}_{\\lambda},\\mathbf{v}_{\\gamma}$ 。我们有$\\lambda\\langle \\mathbf{v}_{\\lambda},\\mathbf{v}_{\\gamma}\\rangle=\\langle \\mathbf{S}\\mathbf{v}_{\\lambda},\\mathbf{v}_{\\gamma}\\rangle=\\langle \\mathbf{v}_{\\lambda},\\mathbf{S}\\mathbf{v}_{\\gamma}\\rangle=\\gamma\\langle \\mathbf{v}_{\\lambda},\\mathbf{v}_{\\gamma}\\rangle\\Rightarrow (\\lambda-\\gamma)\\langle \\mathbf{v}_{\\lambda},\\mathbf{v}_{\\gamma}\\rangle=0$, 因此可知 $\\mathbf{v}_{\\lambda} \\perp\\mathbf{v}_{\\gamma}$。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f336e4-eb7a-4325-8c3f-ecdc1e3c4b42",
   "metadata": {},
   "source": [
    "\n",
    "### 对称矩阵的对角线化\n",
    "\n",
    "对称矩阵的重要性质是，每个特征值对应一个互相垂直的特征子空间，对称矩阵是完全可以对角线化的，不存在一般矩阵的恶劣情况。\n",
    "\n",
    "我们使用数学归纳法，明显 $n=1$ 成立，假设 $n=j$ 成立，对于 $n=j+1$,我们先从一个特征值 $\\lambda$ ，与其对应的特征单位向量 $\\mathbf{v}_{\\lambda}$ 出发。因为  $\\mathbf{A}\\mathbf{v}_{\\lambda}=\\lambda\\mathbf{v}_{\\lambda}$ 因此\n",
    "$$\\begin{pmatrix}\\mathbf{v}_{\\lambda}^T \\\\\n",
    "    (\\mathbf{v}_{\\lambda}^\\perp)^T  \\\\\n",
    " \\end{pmatrix}\\mathbf{A}\\begin{pmatrix}\n",
    "       & \\\\\n",
    "   \\mathbf{v}_{\\lambda} &\\mathbf{v}_{\\lambda}^\\perp   \\\\\n",
    "\t&\\end{pmatrix}=\\begin{pmatrix}\n",
    "       \\lambda& \\mathbf{?}\\\\\n",
    "    \\mathbf{0} &\\mathbf{*}  \n",
    "\\end{pmatrix}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885ccb4a-f253-409d-a208-04a03e86894a",
   "metadata": {},
   "source": [
    "我们观察几件事，首先 \n",
    "$$\\begin{pmatrix}\n",
    "       & \\\\\n",
    "    \\mathbf{v}_{\\lambda} &\\mathbf{v}_{\\lambda}^\\perp  \\\\ &\n",
    "\\end{pmatrix}$$ 是实数单位正交矩阵。而且整体保持对称性，因此我们有\n",
    "$$\\begin{pmatrix}\n",
    "      \\mathbf{v}_{\\lambda}^T \\\\\n",
    "    (\\mathbf{v}_{\\lambda}^\\perp)^T  \\\\\n",
    "\\end{pmatrix}\\mathbf{A}\\begin{pmatrix}\n",
    "       & \\\\\n",
    "    \\mathbf{v}_{\\lambda} &\\mathbf{v}_{\\lambda}^\\perp   \\\\ &\n",
    "\\end{pmatrix}=\\begin{pmatrix}\n",
    "       \\lambda& \\mathbf{0}\\\\\n",
    "    \\mathbf{0} &S_j  \n",
    "\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95db3f18-72c0-4175-bb80-60bfe4b0d702",
   "metadata": {},
   "source": [
    "其中$S_j$ 是一个 $j\\times j$ 的实对称矩阵。对于$\\mathbf{S}_j$ 我们有\n",
    "$$S_j=V_jD_jV^{T}_j$$\n",
    "所以\n",
    "$$\\begin{pmatrix}\n",
    "       \\lambda& \\mathbf{0}\\\\\n",
    "    \\mathbf{0} &S_j  \n",
    "\\end{pmatrix}=\\begin{pmatrix}\n",
    "       1& \\mathbf{0}\\\\\n",
    "    \\mathbf{0} &V_j \n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "       \\lambda& \\mathbf{0}\\\\\n",
    "    \\mathbf{0} &D_j  \n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "       1& \\mathbf{0}\\\\\n",
    "    \\mathbf{0} &V_j^T \n",
    "\\end{pmatrix}$$\n",
    "因此最终对于任意对称实数矩阵$\\mathbf{A}$我们有以下分解。\n",
    "\n",
    "$$\\mathbf{A}=\\mathbf{V}\\mathbf{D}\\mathbf{V}^{T}$$\n",
    "其中 $\\mathbf{D}$ 是一个对角线矩阵，$\\mathbf{V}$ 是一个单位正交矩阵 $\\mathbf{V}^T=\\mathbf{V}^{-1}$。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4020c0ba-fe84-4cf4-a9b2-236093b871bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Chap 10 奇异值分解\n",
    "\n",
    "### 格拉姆矩阵与协方差矩阵\n",
    "\n",
    "实矩阵 $A ： \\mathbb{R}^n \\to \\mathbb{R}^m$ 一般不是个方阵也不对称，如何制造出一个对称的矩阵呢？其实我们在[[点到子空间的距离]]其实已经用过了,对于任意一个实矩阵 $A$ 而言，$A^TA$ 称作 $A$ 的格拉姆矩阵，而$AA^T$ 称之为协方差矩阵，这两者都是实对称矩阵。并且有进一步的性质。\n",
    "\n",
    "对于一般矩阵A来说，其格拉姆矩阵与协方差矩阵的特征值是非负的。对于可逆方阵来说，其格拉姆矩阵与协方差矩阵的特征值是正的。\n",
    "\n",
    "$$\\langle A^TA\\mathbf{v},\\mathbf{v}\\rangle=\\langle A\\mathbf{v},A\\mathbf{v}\\rangle=|A\\mathbf{v}|^2\\geq0,\\;\\forall \\mathbf{v}$$\n",
    "\n",
    "根据[[特征值#对称矩阵的对角线化]]我们有以下分解\n",
    "$$A^TA=V\\Sigma^2V^T$$\n",
    "其中 $V$ 是实单位正交矩阵。$\\Sigma^2=(\\delta_{ij}\\sigma_{ij}^2)$ 是实对角矩阵（方阵）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b6dd27-5a00-4ae7-9d9a-7f313d82db88",
   "metadata": {},
   "source": [
    "\n",
    "### 奇异值分解\n",
    "\n",
    "对于一般矩阵 $A$\n",
    "$A^TA$ 的对角化分解，与$A$ 的奇异值分解有密切的关系。延续上一节的内容，\n",
    "\n",
    "$V$ 的列向量 $\\{\\mathbf{v}_i\\}$ 自然互相垂直，我们首先证明 $\\{A\\mathbf{v}_i\\}$ 也互相垂直。\n",
    "考虑 $$\\langle A\\mathbf{v}_i,A\\mathbf{v}_j\\rangle=\\langle \\mathbf{v}_i,A^TA\\mathbf{v}_j\\rangle=\\langle \\mathbf{v}_i,\\sigma^2_{jj}\\mathbf{v}_j\\rangle=0$$\n",
    "我们将 $\\{A\\mathbf{v}_i\\}$ 单位化，并且用单位垂直向量补足其在 $\\mathbb{R}^m$ 中缺少的部分，成为  $\\{\\mathbf{u}_i\\}$，同时可以获得$\\{\\sigma_i\\}$。我们则有奇异值分解$$A=U\\Sigma V^T$$\n",
    "或\n",
    "\n",
    "$$A=\\sum \\sigma_{ii}  \\mathbf{u}_i \\mathbf{v}_i^T$$\n",
    "其中 $U,V$ 都是单位正交矩阵，或者 unitary matrix（real） = orthogonal matrix = orthonormal matrix（注意$U,V$维数不同）而 $\\Sigma=(\\delta_{ij}\\sigma_{ij})$ 是实对角矩阵（非方阵）。$\\sigma_{11} \\geq \\sigma_{22} \\geq \\sigma_{33}\\dots \\geq 0$ 亦即 $\\Sigma \\geq 0$。\n",
    "\n",
    " 假设我们忽略 $\\mathbf{u}_i, \\mathbf{v}_i$ 可以同时变号,$U,V$ 是唯一的。\n",
    "\n",
    "依据奇异值分解，我们可以计算 $A^TA=V \\Sigma^T U^T U\\Sigma V^T=V \\Sigma \\Sigma V^T$ 。显然 \"$\\Sigma^T \\Sigma \\sim \\Sigma^2$ \"。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74268450-bec0-486c-881f-1a79e0869b00",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 从奇异值分解看方程式解\n",
    "\n",
    "Consider $Ker(A)\\neq\\phi$  :$A\\mathbf x=\\mathbf b$ Too many solutions\n",
    "$Im(A)$ is not full. :$A\\mathbf x=\\mathbf b$ No solutions. can only find best approximation\n",
    "\n",
    "Example \n",
    " $\\hat{I}_{3\\times3} ： \\mathbb{R}^3 \\to \\mathbb{R}^5$,\n",
    " $\\bar{I}_{3\\times3} ： \\mathbb{R}^5 \\to \\mathbb{R}^3$, \n",
    "\n",
    "$A\\mathbf x=\\mathbf b\\Leftrightarrow U\\Sigma V^T \\mathbf x=\\mathbf b\\Leftrightarrow  \\Sigma V^T \\mathbf x=U^T\\mathbf b \\Leftrightarrow \\Sigma  \\mathbf x'=\\mathbf b'$\n",
    "$\\hat{\\mathbf x}=A_{pinv}\\mathbf b$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 与实对称矩阵分解的区别\n",
    "\n",
    "作为一个特殊例子，任何  n × n  实对称矩阵，它们的特征值是实数（可能为负数）\n",
    "As a special case, for every n × n real symmetric matrix, the eigenvalues are real and the eigenvectors can be chosen real and [orthonormal](https://en.wikipedia.org/wiki/Orthonormal). Thus a real symmetric matrix A can be decomposed as $A=Q \\Lambda Q^T$ with $|\\lambda_1| \\geq |\\lambda_2| \\geq |\\lambda_3| \\geq …$ notice that $\\lambda$ could have negative values. However we have $\\lambda_i^2$=$\\sigma_i^2$。\n",
    "\n",
    "### 矩阵压缩\n",
    "\n",
    "矩阵压缩就相当于排除较小的 $\\sigma_{ii}$ 。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf266c5-fb99-4e9c-a8d3-c99ac761c5e9",
   "metadata": {},
   "source": [
    "### 奇异值分解应用\n",
    "\n",
    "**推荐系统** 人 - 物 \n",
    "**脑机界面** 指令 - 讯号 \n",
    "**时空大数据** 点对点交通流量 - 时间（日）\n",
    "\tOrigin - Destination \n",
    "\t问题: 有些地方一天无法到达，需要几天，所以 OD 最好只包含一日能到达的地方 \n",
    "**气候、智慧城市** 点温度变化 - 时间（年、日、时） \n",
    "**Music Genre Classification** Text Style - Melody \n",
    "**Image compressing** - X Y \n",
    "**Video Forensics** - X Y  辨识伪造\n",
    "**Sports Analytics** - Score，Player Data (假设球员球感或低谷期会维持一段时间)\n",
    "**Social Network Analysis** P2P, n 个人 adjacency matrix 连接矩阵\n",
    "还有别的应用，比方说用网路 Chatgpt 可以把非结构化资料结构化。股票报告分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a460a220-b540-406d-9bb5-2fc98a084eb6",
   "metadata": {},
   "source": [
    "### 奇异值分解弱点\n",
    "\n",
    "**非线性**，比方说我们创造一种语言，单词只有三个字母： ABC ABS SOS EVD...  ABC 跟 ABS（塑料）虽然只差了一个英文字母，但意义差很多。\n",
    "\n",
    "神经网路，处理非线性。\n",
    "\n",
    "神经网路，假设现实世界可以用函数表示（可能是个随机函数），是对函数的非线性拟合。但是根据微积分，局部都是线性（有可能加一些噪音）。\n",
    "\n",
    "还是 ABC， ABS的例子。现在可能是手写的 ABC，跟 ABS。在很多 ABS 的不同字迹中间，SVD还是有用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eece242-c1fd-4774-a75c-4d2d575ebba1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
